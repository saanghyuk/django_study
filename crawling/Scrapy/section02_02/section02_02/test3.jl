{"contents": "Hi there,This is the first issue of the  newsletter. Enjoy!- Scrapy plays well with Playwright- The latest Python security fix might have broken your spider"}
{"contents": "Zyte is launching a bi-weekly community newsletter specifically made for web scraping developers. No-fluff. Only coding stuff. .is starting off as a newsletter but we\u2019re planning to grow into something much bigger so developers have a place to learn, discuss web scraping challenges and find answers to their questions in a safe environment.\u00a0As a first step, we are kicking off a bi-weekly coding newsletter for developers who want to level up their knowledge and get the latest trends in the web scraping world.Developers (or managers) who are actively engaged in web data extraction projects."}
{"contents": "Whether we are talking about the credit-granting or insurance underwriting arena, alternative data usually refers to datasets not inherently related to an individual's credit or insurance claim behavior. Traditional data is usually circumscribed to that originating at a credit bureau (think Equifax. Experian, TransUnion), credit or insurance application data, or an institution's proprietary files on an existing customer.Alternative data has a lot of senses become not only a hot topic but even a buzzword, partly because of the data explosion of the last decade (IDC estimated that in 2010 1.2 zettabytes of data were created that year. 2018 saw 33 zettabytes of data made, leading  that in 2025, 175 zettabytes of new data will be produced worldwide). And partly because of the hype that it has received from bullish hedge funds and other organizations that tend to be more prone to experiment with 'new' approaches. Full-time employees dedicated to leveraging alternative data in this space have grown by 450% in the last five years, with 44% of funds now having dedicated teams to reap the benefits of this data alt. data onslaught, according to EY.But the other side of the coin is that there are an estimated 3 billion adults around the globe with no credit, therefore no credit files whatsoever, according to the scoring leader FICO. It is a vast under-serviced market in many parts of the world. While many of these adults live in developing and frontier markets with early-stage credit infrastructures, it is also true that there are large amounts of people in mature markets that have no credit files, thereby unknown to the credit bureaus. These are the so-called 'credit invisibles'.Why being 'credit invisible' matters, one may ask? In the US, for example, one if not the most significant and most mature credit & insurance market, according to the "}
{"contents": "Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions.However, as the number of websites continues to grow, data extraction has become increasingly complex. Constantly changing website structures, antibot practices and scaling requirements add to the difficulty.To overcome the ever-increasing challenges, the web data extraction industry has gone through impressive innovations and transformations in recent years that enable extracting accurate data in an efficient manner. During this revolutionary time in the industry, we are delighted to announce that Scrapinghub will be once again hosting the this year on ."}
{"contents": "Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions.However, as the number of websites continues to grow, data extraction has become increasingly complex. Constantly changing website structures, antibot practices and scaling requirements add to the difficulty.To overcome the ever-increasing challenges, the web data extraction industry has gone through impressive innovations and transformations in recent years that enable extracting accurate data in an efficient manner. During this revolutionary time in the industry, we are delighted to announce that Scrapinghub will be once again hosting the this year on ."}
{"contents": "Over the last decade, we have seen web data grow in importance to the point where it is now essential to all data-driven businesses.\u00a0 Advancements in data analytics and AI continue to drive demand for reliable high-quality web data.Innovation has been at the heart of how we have addressed this challenge. We led the way with open source projects like ,\u00a0  (formerly Crawlera) and our end-to-end ."}
{"contents": "Getting hold of clean, accurate web data \u2013 quickly, and in a format that\u2019s easy to manage \u2013 is a struggle for many organizations. One solution is hiring a couple of enthusiastic interns to copy and paste the information you\u2019re looking for, but they\u2019ll soon be struggling on larger-scale projects. Alternatively, you might use commercially available data extractor software or scraping apps. Or if you\u2019re feeling brave you could try writing your own script for web scraping.\u00a0Let\u2019s imagine you need to get product names or prices from an e-commerce marketplace for comparison purposes. Web scraping traditionally means identifying product pages on a site and then extracting relevant information from those pages.\u00a0To achieve this your developer needs to inspect the site\u2019s source code, and then write some more code to pick out the relevant bits like links, product names, and prices. There are some excellent tools out there to do this, including CSS and XPath selectors or a free open source framework like .\u00a0At Zyte, we believe every business should be able to access the web data it needs, quickly and cost-effectively. Released last year, our  makes it easy to extract structured data from web pages without having to write site-specific code. Just feed URLs of individual pages you want to scrape, and the API serves up your requested data in a standard schema.\u00a0"}
{"contents": "Hi there,This is the first issue of the  newsletter. Enjoy!- Scrapy plays well with Playwright- The latest Python security fix might have broken your spider"}
{"contents": "February 2nd, 2021, Scrapinghub, a global leader in web data extraction technology and services, today becomes . Crawling over 13 billion web pages every month, Zyte continues Scrapinghub\u2019s original mission to give organizations faster, easier access to accurate web data that drives better business decisions.\u201cExtraction needs are changing\u201d says Zyte Founder and CEO Shane Evans. \u201cOrganizations have never been more reliant on data as the differentiator that fuels innovation and business growth. As the web gets bigger and more complex, it\u2019s a challenge for developers to keep pace with changed pages that break spiders, or legitimate extraction efforts that get banned, blocked or blacklisted by overzealous bots.After ten successful years as Scrapinghub we\u2019ve renamed ourselves to Zyte to reflect the exciting opportunities of an evolving digital landscape... and our own commitment to support customers in an increasingly complex data-driven world.\u201dThe latest addition to the Zyte portfolio of managed "}
{"contents": "From inconsistent website layouts that break our extraction logic to badly written HTML, web scraping comes with its share of difficulties. Over the last few years, the single most important challenge in web scraping has been to actually get to the data - and not get blocked. This is due to the antibots or the underlying technologies that websites use to protect their data. Proxies are a major component in any scalable web scraping infrastructure. However, not many people understand the technicalities of the different types of proxies and how to make the best use of proxies to get the data they want, with the least possible blocks.Oftentimes the\u00a0emphasis is on proxies to get around antibots. But the logic of the scraper is important too.\u00a0It is fairly intertwined. Using good quality proxies is surely important. If you use blacklisted proxies, even the best scraper logic will not yield good results.However, a good circumvention logic of the scraper that is in tune with the requirement of the website is equally important. Over the years, antibots have shifted from server-side validation to client-side validation where they look at javascript and browser fingerprinting, etc\u2026So really, it depends a lot on the target website. Most of the time, decent proxies combined with good crawling knowledge and accrual strategy should do the trick and deliver acceptable results.Bans and antibots are primarily designed to prevent the abuse of a website and it is very important to\u00a0"}
{"contents": "When crawling the web, there\u2019s always a speed limit. A spider can't fetch faster than the host willing to send the pages. Page serving takes some amount of resources - CPU, disk, network bandwidth, etc. These resources cost money. Unrestricted serving and extensive crawling are the worst combinations. Such a combination could bring applications to halt and deny service to users. Taking all this into account, limiting serving capacity is natural.This article explains which Scrapy settings help you honor these limits and how to achieve better performance during broad crawls in the presence of these limits.First of all, we need a way to differentiate entities behind domain names. The simplest and fastest one is \"entities never share a single exact domain name\". http://example1.com and http://example2.com are different, so do http://www.example.com and http://about.example.com. Another is \"entities never share a single IP\". Scrapy has to send DNS queries to resolve domain names to IP addresses before deciding. This solves the problem of different domain names served from a single host, so http://www.example.com and http://about.example.com are the same entity.For every entity, there is a slot in a. The number of requests sent to each entity simultaneously is limited by"}
